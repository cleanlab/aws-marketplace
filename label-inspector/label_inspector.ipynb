{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Identify Label Errors with Label Inspector from AWS Marketplace \n",
    "\n",
    "\n",
    "Cleanlab's [Label Inspector](https://aws.amazon.com/marketplace/pp/prodview-en3c45ncm5som) automatically detects label errors in your classification dataset. You just need a (tabular or text) dataset containing class labels and feature values for each datapoint, and this solution will flag examples that are likely mislabeled.\n",
    "\n",
    "This sample notebook demonstrates how to use the Label Inspector via Amazon SageMaker. You can either run it locally from your computer, or from within Sagemaker (recommended).\n",
    "\n",
    "View our handy [AWS Marketplace Guide](../GUIDE.md) if you get stuck anywhere, especially with providing credentials/ARNs or other setup steps.\n",
    "\n",
    "## Pre-requisites\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [Label Inspector](https://aws.amazon.com/marketplace/pp/prodview-en3c45ncm5som). \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to Label Inspector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the Label Inspector offering:\n",
    "1. Open the AWS Marketplace listing page for [Label Inspector](https://aws.amazon.com/marketplace/pp/prodview-en3c45ncm5som).\n",
    "1. On the listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with the EULA and pricing terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify to use this algorithm. Copy the ARN corresponding to your region and specify it in the following cell.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you enter the algorithm ARN in the call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algo_arn = \"<Specify the ARN for Label Inspector obtained from AWS Marketplace>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two code cells below might have a different setup if you are running this sample notebook locally, please check out the [guide to run sample notebooks locally](../GUIDE.md/#run-sample-notebooks-locally) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    session = boto3.Session()\n",
    "    sagemaker_session = sage.Session(session)\n",
    "\n",
    "except ValueError:\n",
    "    # AWS access key id and secret access key only needs to be specified if running notebook locally \n",
    "    # (and AWS credentials were not previously setup)\n",
    "    aws_access_key_id = \"<Specify your AWS Access ID>\"\n",
    "    aws_secret_access_key = \"<Specify your AWS Secret Access Key>\"\n",
    "    region = \"us-east-1\"  # replace with other region if you want, ensure that it matches the region in the ARN\n",
    "    session = boto3.Session(aws_access_key_id, aws_secret_access_key, region_name=region)\n",
    "    sagemaker_session = sage.Session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local variable only needs to be specified if running notebook locally rather than in Sagemaker\n",
    "local_variable_for_sm_role = \"arn:aws:iam::XXXXXX:role/service-role/SageMaker-XXXXX\"  \n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    role = local_variable_for_sm_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define S3 locations for saving data, replace if you would like to store your data in alternative locations\n",
    "bucket_name = sagemaker_session.default_bucket()  # bucket where data will be stored\n",
    "base_folder_name = \"label-inspector\"  # folder inside your bucket where data will be stored\n",
    "\n",
    "training_instance_type = \"ml.m5.xlarge\"  # what type of EC2 instance to use (i.e. how powerful of a computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of EC2 instance will affect how much data can be handled (due to memory limits), how long it takes to return results (ML training takes time), and possibly how accurate the results are. More powerful instances will improve things along all these dimensions.\n",
    "\n",
    "If your dataset contains text fields (strings that are not discrete categories), we recommend a p*-instance that has GPU such that large language models can be fine-tuned on your data. Use of GPU will produce more accurate results for datasets with text.\n",
    "\n",
    "If your dataset is big (over 100k rows), we recommend an instance with lots of memory: \"ml.m5.24xlarge\" if there are no text fields, \"ml.p3.16xlarge\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Prepare dataset and Upload to Amazon S3 (skip if data is already in S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example dataset that you can run Label Inspector on. Label inspector will take approximately 5 minutes to train a ML model and identify label errors on this example dataset.\n",
    "\n",
    "If using your own data, please ensure that the first line of your CSV file is a header with the column names for your data. The first column of your data must be the containing the class labels (remaining columns will be treated as predictive features). Also make sure that the labels are categorical strings (e.g. not continuous numbers, but discrete integers are okay), as only multi-class (and binary) classification datasets are supported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>69</td>\n",
       "      <td>86</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>76</td>\n",
       "      <td>85</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>51</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter_grade  exam_1  exam_2  exam_3 notes\n",
       "0            C      69      86      75   NaN\n",
       "1            A      76      85      74   NaN\n",
       "2            B      87      81      86   NaN\n",
       "3            C      47      77      88   NaN\n",
       "4            C      51      78      84   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/input/dataset.csv\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset = \"data/input/dataset.csv\"  # replace filepath here if using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload data to S3\n",
    "input_folder = \"{}/{}/{}\".format(base_folder_name, \"train\", \"input\")\n",
    "training_data_location = sagemaker_session.upload_data(training_dataset, bucket=bucket_name, key_prefix=input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can find your data here after uploading\n",
    "training_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a ML model to analyze the labels in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring that our data is an accessible Amazon S3 bucket (only read permissions are required for the algorithm to access the input data bucket), we are ready to train a machine learning model. This model can be automatically trained on diverse types of tabular/text data via state-of-the-art AutoML, and is used to identify which labels are likely incorrect. \n",
    "\n",
    "In the code cell below we specify the S3 location of our data. If you have followed the [Upload datasets to Amazon S3](#Upload-datasets-to-Amazon-S3) section of this tutorial, the S3 location should already be specified. If you are using your own dataset, make sure to specify the location of your data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data_location  # replace with the S3 URI of your data if using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the output folder location based on variables specified above\n",
    "# this is boilerplate code and does not need to be edited\n",
    "output_location = \"s3://{}/{}/{}/{}\".format(bucket_name, base_folder_name, \"train\", \"output\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the hyperparameters for our algorithm. Currently, our algorithm only supports one hyperparameter `runtime` with two options:\n",
    "\n",
    "- `fast` will have a shorter execution time, but may not produce the best quality results (maximum execution time: 2 hours, will take much less time for most datasets)\n",
    "- `high_accuracy` will be slower, but produces high quality results (maximum execution time: 13 hours, will take much less time for most datasets)\n",
    "\n",
    "In either case, you can get results faster by specifying a more powerful `instance_type` (and the results may be more accurate). When estimating costs, keep in mind that a more powerful instance can run the job faster (and has more memory available which may be required for larger datasets). ML training scales proportionally to the size of your dataset and may take some time, so just keep this job running and check back later to see if it has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\"runtime\": \"high_accuracy\"}  # change to \"fast\" to get quicker results (will be less accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create an estimator object for running a training job and train our model. For information on creating an `Estimator` object, check out the [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"label-inspector\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: label-inspector-2023-05-03-03-52-24-819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 03:52:25 Starting - Starting the training job...\n",
      "2023-05-03 03:52:40 Starting - Preparing the instances for training...\n",
      "2023-05-03 03:53:34 Downloading - Downloading input data\n",
      "2023-05-03 03:53:34 Training - Downloading the training image.....................\n",
      "2023-05-03 03:56:51 Training - Training image download completed. Training in progress..\n",
      "\u001b[34mAnalyzing your data ... the estimated maximum runtime (upper bound) is: 13 hours\u001b[0m\n",
      "\n",
      "2023-05-03 03:58:36 Uploading - Uploading generated training model\n",
      "\u001b[34mLabel errors found.\u001b[0m\n",
      "\n",
      "2023-05-03 03:58:52 Completed - Training job completed\n",
      "Training seconds: 333\n",
      "Billable seconds: 333\n"
     ]
    }
   ],
   "source": [
    "# run the training job\n",
    "estimator.fit({\"training\": training_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your output will be available on following path\n",
    "output_file_location = os.path.join(output_location, estimator._current_job_name, \"output\")\n",
    "output_file_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Model Training Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get results faster (but potentially less accurate estimates): Specify a more powerful instance type and set `hyperparameters = {\"runtime\": \"fast\"}`. You can also try subsampling your dataset to a smaller one for a quick trial run (although be aware the ML training -- and hence accuracy of estimated label issues -- will become worse when the dataset is small). \n",
    "\n",
    "To get more accurate results: Specify a more powerful instance type. If you have text fields in your datasetÂ (i.e. strings that are not discrete categories), use an instance that has a GPU (like a p-instance or g-instance) so large language models can be fine-tuned on your data. Also set `hyperparameters = {\"runtime\": \"high_accuracy\"}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fetch results of the label analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training job completes, we can get the results output by this analysis from the S3 bucket specified above. The main output of this solution is a CSV file with information about each label in your dataset. To learn more about what each column of the output file contains, check out our documentation [here](README.md#output). The examples flagged as likely mislabeled with the lowest label quality scores are the ones whose labels you should review closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = session.client('s3')\n",
    "\n",
    "# create local folder to store output files\n",
    "if not os.path.exists('data/output'):\n",
    "    os.makedirs('data/output')\n",
    "\n",
    "# downloading the file from S3\n",
    "with open('data/output/output.tar.gz', 'wb') as f:\n",
    "    s3.download_fileobj(bucket_name, f\"{base_folder_name}/train/output/{estimator._current_job_name}/output/output.tar.gz\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tarfile.open('data/output/output.tar.gz') as file:\n",
    "    file.extractall('data/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.675171</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.087378</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.751706</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.768885</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.531283</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_label_issue  label_score given_label predicted_label\n",
       "0           False     0.675171           C               C\n",
       "1            True     0.087378           A               C\n",
       "2           False     0.751706           B               B\n",
       "3           False     0.768885           C               C\n",
       "4           False     0.531283           C               C"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanset = pd.read_csv(\"data/output/cleanset.csv\")\n",
    "cleanset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the returned cleanset corresponds to a row in your original dataset. You can concatenate these two files into one to better review the identified label errors. We show an example of how to view the top errors in our sample dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = pd.read_csv(\"data/input/dataset.csv\")  # replace filepath here if using your own dataset\n",
    "merged_cleanset = pd.concat([original_dataset, cleanset], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can view the top label errors that was identified by filtering for the examples where `is_label_issue` is `True`, and sorting them by their `label_score` (lower score indicates more errorneous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>B</td>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>A</td>\n",
       "      <td>78</td>\n",
       "      <td>95</td>\n",
       "      <td>54</td>\n",
       "      <td>missed homework frequently -10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>True</td>\n",
       "      <td>0.024922</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>A</td>\n",
       "      <td>95</td>\n",
       "      <td>87</td>\n",
       "      <td>82</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.025457</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    letter_grade  exam_1  exam_2  exam_3                           notes  \\\n",
       "208            F     100     100      99                             NaN   \n",
       "92             B      77      51      70                             NaN   \n",
       "77             A      78      95      54  missed homework frequently -10   \n",
       "338            A       0      79      65      cheated on exam, gets 0pts   \n",
       "472            A      95      87      82     missed class frequently -10   \n",
       "\n",
       "     is_label_issue  label_score given_label predicted_label  \n",
       "208            True     0.012362           F               A  \n",
       "92             True     0.015489           B               D  \n",
       "77             True     0.017577           A               D  \n",
       "338            True     0.024922           A               F  \n",
       "472            True     0.025457           A               C  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_errors = merged_cleanset[merged_cleanset[\"is_label_issue\"] == True].sort_values(\"label_score\")\n",
    "label_errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy model to make predictions and find label errors on new data (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Inspector trains a robust ML model to identify potential label errors in your dataset. After the training is completed, you can deploy this trained model to classify any new data that you get. If your new data has accompanying labels, Label Inspector will also identify any potential label errors in the new data.\n",
    "\n",
    "Please note that the identification of label issues in your new data is done using models and statistics fitted to the training data.  If the distribution of your data is evolving and you have enough time to re-fit models, then you may better identify label errors by merging your new data with your original training dataset and feeding this into a new Label Inspector training job.\n",
    "\n",
    "This section is optional if you only want to detect label errors in your original training dataset (all results on label errors in the trainset were returned at the end of the training job)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch inference is useful for getting predictions for large amounts of data where there is less urgency. It also allows multiple input payloads at a time (usually stored in S3). Learn more about batch inference from the [AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_transform_inference_instance_type = \"ml.m5.xlarge\"  # replace with a larger instance if necessary (consider GPU instance if you used a GPU instance for the training job) \n",
    "content_type = \"text/csv\"  # label inspector only supports csv file, do not change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch transform jobs can take in multiple payloads, we upload data from a folder (instead of an individual file) to an S3 folder. We will then pass in that S3 folder to the batch transform job which performs inference on all files in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of local folder that we want to upload to S3 and download results to\n",
    "# replace this local folder if you have your data stored somewhere else\n",
    "transform_input_folder = \"data/input/batch_inference\"  \n",
    "transform_output_folder = \"data/output/batch_inference\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input and output folders paths on S3\n",
    "transform_input_path = \"{}/{}/{}\".format(base_folder_name, \"inference\", \"input\")\n",
    "transform_output_path = \"s3://{}/{}/{}/{}\".format(bucket_name, base_folder_name, \"inference\", \"output\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show two sample files that we will be using for inference. One of the inference datasets does not contain any labels (we want predicted labels for this data), and the other dataset contains labels (we want to find label errors).\n",
    "\n",
    "If using your own data, please ensure that the data passed to the inference job has the same columns as the training data (the only column that is optional is the label column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>46</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>99</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>77</td>\n",
       "      <td>great final presentation +10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_1  exam_2  exam_3                         notes\n",
       "0      84      92      86                           NaN\n",
       "1      84      46      95                           NaN\n",
       "2       0      87      99    cheated on exam, gets 0pts\n",
       "3      92      93      81                           NaN\n",
       "4      74      73      77  great final presentation +10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is a sample dataset for inference, notice how the columns are the same as the training data\n",
    "inference_dataset = pd.read_csv(f\"{transform_input_folder}/batch_dataset.csv\")\n",
    "inference_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>84</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>84</td>\n",
       "      <td>46</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>99</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>77</td>\n",
       "      <td>great final presentation +10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter_grade  exam_1  exam_2  exam_3                         notes\n",
       "0            B      84      92      86                           NaN\n",
       "1            C      84      46      95                           NaN\n",
       "2            D       0      87      99    cheated on exam, gets 0pts\n",
       "3            B      92      93      81                           NaN\n",
       "4            B      74      73      77  great final presentation +10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is another sample dataset for inference that contains the labels\n",
    "inference_dataset = pd.read_csv(f\"{transform_input_folder}/batch_dataset_with_labels.csv\")\n",
    "inference_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we upload the data in our specified local folder to a S3 folder. Our local folder (specified using `transform_input_folder`) contains two CSV files that we want to perform batch transform on. Note that you can have as many files as you want in that folder as long as they are valid files to perform batch transform on.\n",
    "\n",
    "Skip this step if your data is already in S3, and just specify where your data is in S3 using the `transform_input_location` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this line if data is already in S3, and instead define transform_input_location as the S3 folder containing them \n",
    "transform_input_location = sagemaker_session.upload_data(transform_input_folder, bucket=bucket_name, key_prefix=transform_input_path)\n",
    "\n",
    "print(\"Transform input uploaded to \" + transform_input_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring that your data is on S3, you can run the batch transform by passing in the S3 location of where your data is stored. The batch transform will save the results of the batch transform to an output folder in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(1, batch_transform_inference_instance_type, output_path=transform_output_path)\n",
    "transformer.transform(transform_input_location, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the batch transform job is complete, we can get the output data from the S3 bucket specified above. Here, we will download the entire S3 folder, so if you had more than one input files, the follow code cell will download all the corresponding output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource = session.resource('s3')\n",
    "resource_bucket = resource.Bucket(bucket_name)\n",
    "\n",
    "# create local folder to store output files\n",
    "if not os.path.exists(transform_output_folder):\n",
    "   os.makedirs(transform_output_folder)\n",
    "\n",
    "# download files from S3\n",
    "for object in resource_bucket.objects.filter(Prefix=f'{base_folder_name}/inference/output/'):\n",
    "    filename = object.key.split(\"/\")[-1]\n",
    "    resource_bucket.download_file(object.key, f\"{transform_output_folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we show a sample of what the output predictions returned by batch inference will look like. \n",
    "\n",
    "If the inference dataset does not contain any labels, the output contains two pieces of information:\n",
    "\n",
    "- the first column contains the predicted label for each example (rows are in the same order as your inference dataset) \n",
    "- the remaining columns contain the predicted class probabilities for each example, these columns will have the names `[CLASS_NAME]_probability`\n",
    "    - in our sample dataset, the `A_probability` column contains the model predicted probability that each example belongs to class A (i.e. that this student should have the letter grade A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>A_probability</th>\n",
       "      <th>B_probability</th>\n",
       "      <th>C_probability</th>\n",
       "      <th>D_probability</th>\n",
       "      <th>F_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.062658</td>\n",
       "      <td>0.711071</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>0.136640</td>\n",
       "      <td>0.075542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>0.045886</td>\n",
       "      <td>0.136571</td>\n",
       "      <td>0.638398</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.046003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>0.028875</td>\n",
       "      <td>0.172331</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.774074</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.079583</td>\n",
       "      <td>0.567199</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.229104</td>\n",
       "      <td>0.110115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.118505</td>\n",
       "      <td>0.599401</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>0.114506</td>\n",
       "      <td>0.141864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted_label  A_probability  B_probability  C_probability  D_probability  \\\n",
       "0               B       0.062658       0.711071       0.014089       0.136640   \n",
       "1               C       0.045886       0.136571       0.638398       0.133141   \n",
       "2               D       0.028875       0.172331       0.009094       0.774074   \n",
       "3               B       0.079583       0.567199       0.013999       0.229104   \n",
       "4               B       0.118505       0.599401       0.025725       0.114506   \n",
       "\n",
       "   F_probability  \n",
       "0       0.075542  \n",
       "1       0.046003  \n",
       "2       0.015625  \n",
       "3       0.110115  \n",
       "4       0.141864  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transform_preds = pd.read_csv(f\"{transform_output_folder}/batch_dataset.csv.out\")\n",
    "batch_transform_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the inference dataset contains a label column, the output results will contain two extra columns:\n",
    "\n",
    "- `is_label_issue`, a boolean value specifying whether a label is identified as an error\n",
    "- `label_score`, a quality scores estimating the likelihood that each example is correctly labeled (lower scores indicate noisier labels)\n",
    "\n",
    "Below is a sample of the output returned by batch inference for datasets with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>A_probability</th>\n",
       "      <th>B_probability</th>\n",
       "      <th>C_probability</th>\n",
       "      <th>D_probability</th>\n",
       "      <th>F_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>0.711071</td>\n",
       "      <td>0.062658</td>\n",
       "      <td>0.711071</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>0.136640</td>\n",
       "      <td>0.075542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>0.638398</td>\n",
       "      <td>0.045886</td>\n",
       "      <td>0.136571</td>\n",
       "      <td>0.638398</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.046003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>0.774074</td>\n",
       "      <td>0.028875</td>\n",
       "      <td>0.172331</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.774074</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>0.567199</td>\n",
       "      <td>0.079583</td>\n",
       "      <td>0.567199</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.229104</td>\n",
       "      <td>0.110115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>0.599401</td>\n",
       "      <td>0.118505</td>\n",
       "      <td>0.599401</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>0.114506</td>\n",
       "      <td>0.141864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted_label  is_label_issue  label_score  A_probability  B_probability  \\\n",
       "0               B           False     0.711071       0.062658       0.711071   \n",
       "1               C           False     0.638398       0.045886       0.136571   \n",
       "2               D           False     0.774074       0.028875       0.172331   \n",
       "3               B           False     0.567199       0.079583       0.567199   \n",
       "4               B           False     0.599401       0.118505       0.599401   \n",
       "\n",
       "   C_probability  D_probability  F_probability  \n",
       "0       0.014089       0.136640       0.075542  \n",
       "1       0.638398       0.133141       0.046003  \n",
       "2       0.009094       0.774074       0.015625  \n",
       "3       0.013999       0.229104       0.110115  \n",
       "4       0.025725       0.114506       0.141864  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transform_preds = pd.read_csv(f\"{transform_output_folder}/batch_dataset_with_labels.csv.out\")\n",
    "batch_transform_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With real-time inference, you can deploy an endpoint that will allow you to get predictions for new datapoints immediately.\n",
    "Learn more about real-time SageMaker inference from the [AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define instance type and endpoint name\n",
    "real_time_inference_instance_type = \"ml.m5.xlarge\"  # replace with a large instance if necessary\n",
    "endpoint_name = \"label-inspector-predictor\"  # note that endpoint names have to be unique\n",
    "\n",
    "content_type = \"text/csv\"  # label inspector only supports csv file, do not change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model package with name: label-inspector-2023-05-03-04-08-14-940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: label-inspector-2023-05-03-04-08-14-940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name label-inspector-predictor\n",
      "INFO:sagemaker:Creating endpoint with name label-inspector-predictor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=real_time_inference_instance_type, \n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your endpoint is deployed, we can run real-time inference on a dataset to get the predicted labels.\n",
    "\n",
    "Here we will use an example dataset that we want to perform inference on. If using your own data, please ensure that the data passed to the inference job has the same columns as the training data (the only column that is not needed is the label column). \n",
    "\n",
    "Note that real-time inference is used to get predictions for new datapoints instantaneously. Inputs to real-time inference should only contain small numbers of datapoints. If you have a big dataset, please subset your data into smaller groups to pass as inputs to the real-time inference endpoint (you can pass in these groups one after another), or use batch inference demonstrated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the input and output filepaths\n",
    "real_time_input_file = \"data/input/inference_dataset_with_labels.csv\"  # replace filepath if using your own dataset\n",
    "real_time_output_file = \"data/output/predictions_with_labels.csv\"  # replace filepath if you want to save the outputs somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"label-inspector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>95</td>\n",
       "      <td>great participation +10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>97</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>great final presentation +10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter_grade  exam_1  exam_2  exam_3                         notes\n",
       "0            B      87      83      78                           NaN\n",
       "1            F      79      77      95       great participation +10\n",
       "2            A      97      94      87  great final presentation +10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is a sample dataset for inference, notice how the columns are the same as the training data\n",
    "inference_dataset = pd.read_csv(real_time_input_file)\n",
    "inference_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can invoke the deployed endpoint and save the results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime = session.client('sagemaker-runtime')\n",
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "# read file into memory\n",
    "with open(real_time_input_file, encoding=\"utf-8\") as f:\n",
    "    data_input = f.read()\n",
    "    \n",
    "# invoke endpoint to perform inference\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='text/csv', Body=data_input)\n",
    "\n",
    "# create folder to store output files\n",
    "if not os.path.exists('data/output'):\n",
    "    os.makedirs('data/output')\n",
    "    \n",
    "# unpack response and save to file\n",
    "with open(real_time_output_file, 'wb') as file:\n",
    "    file.write(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can also invoke the endpoint with the following CLI command:\n",
    "\n",
    "```shell\n",
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name $endpoint_name --body fileb://$input_file_name --content-type $content_type --region $sagemaker_session.boto_region_name $output_file_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us view the label predictions and predicted class probabilities from our real-time inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>A_probability</th>\n",
       "      <th>B_probability</th>\n",
       "      <th>C_probability</th>\n",
       "      <th>D_probability</th>\n",
       "      <th>F_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>0.611334</td>\n",
       "      <td>0.028280</td>\n",
       "      <td>0.611334</td>\n",
       "      <td>0.111949</td>\n",
       "      <td>0.172019</td>\n",
       "      <td>0.076418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>True</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>0.802403</td>\n",
       "      <td>0.066521</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>0.109769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0.966910</td>\n",
       "      <td>0.966910</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.025167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted_label  is_label_issue  label_score  A_probability  B_probability  \\\n",
       "0               B           False     0.611334       0.028280       0.611334   \n",
       "1               A            True     0.109769       0.802403       0.066521   \n",
       "2               A           False     0.966910       0.966910       0.005799   \n",
       "\n",
       "   C_probability  D_probability  F_probability  \n",
       "0       0.111949       0.172019       0.076418  \n",
       "1       0.014118       0.007190       0.109769  \n",
       "2       0.001136       0.000989       0.025167  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_time_preds = pd.read_csv(real_time_output_file)\n",
    "real_time_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, the output contains the following information:\n",
    "\n",
    "- a predicted label for each example (rows are in the same order as your inference dataset) \n",
    "- the predicted class probabilities for each example, these columns will have the names `[CLASS_NAME]_probability`\n",
    "    - in our sample dataset, the `A_probability` column contains the model predicted probability that each example belongs to class A (i.e. that this student should have the letter grade A).\n",
    "    \n",
    "Additionally, if your input inference dataset has a label column (which our demo dataset does), we also perform label error analysis and return the following columns:\n",
    "\n",
    "- `is_label_issue`, a boolean value specifying whether a label is identified as an error\n",
    "- `label_score`, a quality scores estimating the likelihood that each example is correctly labeled (lower scores indicate noisier labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Clean Up Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete models endpoints (skip if you skipped Section 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are done with performing real-time inferences, you no longer need the saved model and deployed endpoint. Here we show how to terminate the endpoint to avoid being charged.\n",
    "\n",
    "Alternatively, you can navigate to the [SageMaker Console](https://console.aws.amazon.com/sagemaker) and manually delete the endpoints yourself. The endpoint and endpoint configuration can be found under **Inference > Endpoints** and **Inference > Endpoint configurations** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also delete the model associated with the batch transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformer.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete S3 resources (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains the code that can be used to automatically delete the files created in S3 in this sample notebook. Be wary that this command will delete all the files stored in the `base_folder_name` folder specified above. **Proceed with caution** if you have previously stored other files in that folder.\n",
    "\n",
    "Alternatively, you can navigate to the [S3 Management Console](https://s3.console.aws.amazon.com/) and manually delete these data files yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resource = session.resource('s3')\n",
    "# resource_bucket = resource.Bucket(bucket_name)\n",
    "\n",
    "# # before executing the code below, ensure there is nothing else important in this folder\n",
    "# resource_bucket.objects.filter(Prefix=f\"{base_folder_name}/\").delete()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ask questions or report problems, please email: support@cleanlab.ai\n",
    "\n",
    "To run a more in-depth analysis of your data and labels, try [Cleanlab Studio](https://cleanlab.ai/studio/) for free (it supports image data as well)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
