{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Identify Label Errors with Label Inspector from AWS Marketplace \n",
    "\n",
    "\n",
    "Cleanlab's [Label Inspector](https://aws.amazon.com/marketplace/pp/prodview-rlbhc2lxttdio) automatically detects label errors in your classification dataset. You just need a (tabular or text) dataset containing class labels and feature values for each datapoint, and this solution will flag examples that are likely mislabeled.\n",
    "\n",
    "This sample notebook demonstrates how to use the Label Inspector via Amazon SageMaker. You can either run it locally from your computer, or from within Sagemaker (recommended).\n",
    "\n",
    "View our handy [AWS Marketplace Guide](../GUIDE.md) if you get stuck anywhere, especially with providing credentials/ARNs or other setup steps.\n",
    "\n",
    "## Pre-requisites\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [Label Inspector](https://aws.amazon.com/marketplace/pp/prodview-rlbhc2lxttdio). \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to Label Inspector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the Label Inspector offering:\n",
    "1. Open the AWS Marketplace listing page for [Label Inspector](https://aws.amazon.com/marketplace/pp/prodview-rlbhc2lxttdio).\n",
    "1. On the listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with the EULA and pricing terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify to use this algorithm. Copy the ARN corresponding to your region and specify it in the following cell.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you enter the algorithm ARN in the call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algo_arn = \"<Specify the ARN for Label Inspector obtained from AWS Marketplace>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two code cells below might have a different setup if you are running this sample notebook locally, please check out the [guide to run sample notebooks locally](../GUIDE.md/#run-sample-notebooks-locally) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    session = boto3.Session()\n",
    "    sagemaker_session = sage.Session(session)\n",
    "\n",
    "except ValueError:\n",
    "    # AWS access key id and secret access key only needs to be specified if running notebook locally \n",
    "    # (and AWS credentials were not previously setup)\n",
    "    aws_access_key_id = \"<Specify your AWS Access ID>\"\n",
    "    aws_secret_access_key = \"<Specify your AWS Secret Access Key>\"\n",
    "    region = \"us-east-1\"  # replace with other region if you want, ensure that it matches the region in the ARN\n",
    "    session = boto3.Session(aws_access_key_id, aws_secret_access_key, region_name=region)\n",
    "    sagemaker_session = sage.Session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local variable only needs to be specified if running notebook locally rather than in Sagemaker\n",
    "local_variable_for_sm_role = \"arn:aws:iam::XXXXXX:role/service-role/SageMaker-XXXXX\"  \n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    role = local_variable_for_sm_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define S3 locations for saving data, replace if you would like to store your data in alternative locations\n",
    "bucket_name = sagemaker_session.default_bucket()  # bucket where data will be stored\n",
    "base_folder_name = \"label-inspector\"  # folder inside your bucket where data will be stored\n",
    "\n",
    "training_instance_type = \"ml.m5.xlarge\"  # what type of EC2 instance to use (i.e. how powerful of a computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of EC2 instance will affect how much data can be handled (due to memory limits), how long it takes to return results (ML training takes time), and possibly how accurate the results are. More powerful instances will improve things along all these dimensions.\n",
    "\n",
    "If your dataset contains text fields (strings that are not discrete categories), we recommend a p*-instance that has GPU such that large language models can be fine-tuned on your data. Use of GPU will produce more accurate results for datasets with text.\n",
    "\n",
    "If your dataset is big (over 100k rows), we recommend an instance with lots of memory: \"ml.m5.24xlarge\" if there are no text fields, \"ml.p3.16xlarge\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Prepare dataset and Upload to Amazon S3 (skip if data is already in S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example dataset that you can run Label Inspector on. Label inspector will take approximately 5 minutes to train a ML model and identify label errors on this example dataset.\n",
    "\n",
    "If using your own data, please ensure that the first line of your CSV file is a header with the column names for your data. The first column of your data must be the containing the class labels (remaining columns will be treated as predictive features). Also make sure that the labels are categorical strings (e.g. not continuous numbers, but discrete integers are okay), as only multi-class (and binary) classification datasets are supported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>great participation +10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter_grade  exam_1  exam_2  exam_3                    notes\n",
       "0            C      53      77      93                      NaN\n",
       "1            B      81      64      80  great participation +10\n",
       "2            B      74      88      97                      NaN\n",
       "3            C      61      94      78                      NaN\n",
       "4            C      48      90      91                      NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/train/input/train_dataset.csv\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset = \"data/train/input/train_dataset.csv\"  # replace filepath here if using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload data to S3\n",
    "input_folder = \"{}/{}/{}\".format(base_folder_name, \"train\", \"input\")\n",
    "training_data_location = sagemaker_session.upload_data(training_dataset, bucket=bucket_name, key_prefix=input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can find your data here after uploading\n",
    "training_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a ML model to analyze the labels in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring that our data is an accessible Amazon S3 bucket (only read permissions are required for the algorithm to access the input data bucket), we are ready to train a machine learning model. This model can be automatically trained on diverse types of tabular/text data via state-of-the-art AutoML, and is used to identify which labels are likely incorrect. \n",
    "\n",
    "In the code cell below we specify the S3 location of our data. If you have followed the [Upload datasets to Amazon S3](#Upload-datasets-to-Amazon-S3) section of this tutorial, the S3 location should already be specified. If you are using your own dataset, make sure to specify the location of your data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data_location  # replace with the S3 URI of your data if using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the output folder location based on variables specified above\n",
    "# this is boilerplate code and does not need to be edited\n",
    "output_location = \"s3://{}/{}/{}/{}\".format(bucket_name, base_folder_name, \"train\", \"output\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the hyperparameters for our algorithm. Currently, our algorithm only supports one hyperparameter `runtime` with two options:\n",
    "\n",
    "- `fast` will have a shorter execution time, but may not produce the best quality results (maximum execution time: 2 hours, will take much less time for most datasets)\n",
    "- `high_accuracy` will be slower, but produces high quality results (maximum execution time: 13 hours, will take much less time for most datasets)\n",
    "\n",
    "In either case, you can get results faster by specifying a more powerful `instance_type` (and the results may be more accurate). When estimating costs, keep in mind that a more powerful instance can run the job faster (and has more memory available which may be required for larger datasets). ML training scales proportionally to the size of your dataset and may take some time, so just keep this job running and check back later to see if it has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\"runtime\": \"high_accuracy\"}  # change to \"fast\" to get quicker results (will be less accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create an estimator object for running a training job and train our model. For information on creating an `Estimator` object, check out the [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"label-inspector\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: label-inspector-2023-03-24-02-28-53-009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-24 02:28:53 Starting - Starting the training job...\n",
      "2023-03-24 02:29:07 Starting - Preparing the instances for training...\n",
      "2023-03-24 02:29:49 Downloading - Downloading input data...\n",
      "2023-03-24 02:30:14 Training - Downloading the training image.........\n",
      "2023-03-24 02:31:50 Training - Training image download completed. Training in progress...\u001b[34mAnalyzing your data ... the estimated maximum runtime (upper bound) is: 13 hours\u001b[0m\n",
      "\u001b[34mLabel errors found.\u001b[0m\n",
      "\n",
      "2023-03-24 02:33:46 Uploading - Uploading generated training model\n",
      "2023-03-24 02:33:46 Completed - Training job completed\n",
      "Training seconds: 238\n",
      "Billable seconds: 238\n"
     ]
    }
   ],
   "source": [
    "# run the training job\n",
    "estimator.fit({\"training\": training_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-043170249292/label-inspector/train/output/label-inspector-2023-03-24-02-28-53-009/output'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your output will be available on following path\n",
    "output_file_location = os.path.join(output_location, estimator._current_job_name, \"output\")\n",
    "output_file_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fetch results of the label analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training job completes, we can get the results output by this analysis from the S3 bucket specified above. The main output of this solution is a CSV file with information about each label in your dataset. To learn more about what each column of the output file contains, check out our documentation [here](README.md#output). The examples flagged as likely mislabeled with the lowest label quality scores are the ones whose labels you should review closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = session.client('s3')\n",
    "\n",
    "# downloading the file from S3\n",
    "with open('data/train/output/output.tar.gz', 'wb') as f:\n",
    "    s3.download_fileobj(bucket_name, f\"{base_folder_name}/train/output/{estimator._current_job_name}/output/output.tar.gz\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tarfile.open('data/train/output/output.tar.gz') as file:\n",
    "    file.extractall('data/train/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.672778</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.691504</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.729446</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.662407</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.458495</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>False</td>\n",
       "      <td>0.377611</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>False</td>\n",
       "      <td>0.417643</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>False</td>\n",
       "      <td>0.733176</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>False</td>\n",
       "      <td>0.755515</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>False</td>\n",
       "      <td>0.593443</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>941 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     is_label_issue  label_score given_label predicted_label\n",
       "0             False     0.672778           C               C\n",
       "1             False     0.691504           B               B\n",
       "2             False     0.729446           B               B\n",
       "3             False     0.662407           C               C\n",
       "4             False     0.458495           C               C\n",
       "..              ...          ...         ...             ...\n",
       "936           False     0.377611           F               F\n",
       "937           False     0.417643           F               F\n",
       "938           False     0.733176           F               F\n",
       "939           False     0.755515           B               B\n",
       "940           False     0.593443           C               C\n",
       "\n",
       "[941 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanset = pd.read_csv(\"data/train/output/cleanset.csv\")\n",
    "cleanset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the returned cleanset corresponds to a row in your original dataset. You can concatenate these two files into one to better review the identified label errors. We show an example of how to view the top errors in our sample dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = pd.read_csv(\"data/train/input/train_dataset.csv\")  # replace filepath here if using your own dataset\n",
    "merged_cleanset = pd.concat([original_dataset, cleanset], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can view the top label errors that was identified by filtering for the examples where `is_label_issue` is `True`, and sorting them by their `label_score` (lower score indicates more errorneous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_score</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>F</td>\n",
       "      <td>98</td>\n",
       "      <td>51</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>A</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>True</td>\n",
       "      <td>0.014680</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>B</td>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>F</td>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.018192</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>F</td>\n",
       "      <td>89</td>\n",
       "      <td>77</td>\n",
       "      <td>68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    letter_grade  exam_1  exam_2  exam_3                       notes  \\\n",
       "318            F      98      51      74                         NaN   \n",
       "705            A      97       0      90  cheated on exam, gets 0pts   \n",
       "689            B      77      51      70                         NaN   \n",
       "597            F      81     100      74                         NaN   \n",
       "619            F      89      77      68                         NaN   \n",
       "\n",
       "     is_label_issue  label_score given_label predicted_label  \n",
       "318            True     0.012267           F               C  \n",
       "705            True     0.014680           A               D  \n",
       "689            True     0.015814           B               D  \n",
       "597            True     0.018192           F               B  \n",
       "619            True     0.022600           F               C  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_errors = merged_cleanset[merged_cleanset[\"is_label_issue\"] == True].sort_values(\"label_score\")\n",
    "label_errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy trained model and perform real time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label inspector trains a ML model to identify potential label errors. After the training is completed, you can deploy the trained model as an endpoint and perform real time inference on any new data that you have. \n",
    "\n",
    "If you want to understand how real-time inference with Amazon SageMaker works, check out this [AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define instance type and endpoint name\n",
    "real_time_inference_instance_type = \"ml.m5.xlarge\"  # replace with a large instance if necessary\n",
    "endpoint_name = \"label-inspector-predictor\"  # note that endpoint names have to be unique\n",
    "\n",
    "content_type = \"text/csv\"  # label inspector only supports csv file, do not change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model package with name: label-inspector-2023-03-24-02-34-09-683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: label-inspector-2023-03-24-02-34-09-683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name label-inspector-predictor\n",
      "INFO:sagemaker:Creating endpoint with name label-inspector-predictor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=real_time_inference_instance_type, \n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your endpoint is deployed, we can run real time inference on a dataset to get the predicted labels.\n",
    "\n",
    "Here we will use an example dataset that we want to perform inference on. If using your own data, please ensure that the data passed to the inference job has the same columns as the training data (the only column that is not needed is the label column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the input and output filepaths\n",
    "real_time_input_file = \"data/real_time_inference/input/inference_dataset.csv\"  # replace filepath if using your own dataset\n",
    "real_time_output_file = \"data/real_time_inference/output/predictions.csv\"  # replace filepath if you want to save the outputs somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "      <td>great final presentation +10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>great participation +10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>63</td>\n",
       "      <td>91</td>\n",
       "      <td>missed homework frequently -10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_1  exam_2  exam_3                           notes\n",
       "0      82      69      75    great final presentation +10\n",
       "1      94      90      90         great participation +10\n",
       "2      92      63      91  missed homework frequently -10\n",
       "3      91       0      68      cheated on exam, gets 0pts\n",
       "4      81      56      58                             NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the sample dataset for inference, notice how the columns are the same as the training data\n",
    "inference_dataset = pd.read_csv(real_time_input_file)\n",
    "inference_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can invoke the deployed endpoint and save the results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime = session.client('sagemaker-runtime')\n",
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "# read file into memory\n",
    "with open(real_time_input_file, encoding=\"utf-8\") as f:\n",
    "    data_input = f.read()\n",
    "    \n",
    "# invoke endpoint to perform inference\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='text/csv', Body=data_input)\n",
    "\n",
    "# unpack response and save to file\n",
    "with open(real_time_output_file, 'wb') as file:\n",
    "    file.write(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can also invoke the endpoint with the following CLI command:\n",
    "\n",
    "```shell\n",
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name $endpoint_name --body fileb://$input_file_name --content-type $content_type --region $sagemaker_session.boto_region_name $output_file_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us view the predictions from our real time inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter_grade\n",
       "0            B\n",
       "1            A\n",
       "2            C\n",
       "3            F\n",
       "4            D"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_time_preds = pd.read_csv(real_time_output_file)\n",
    "real_time_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform batch inference using our trained model, which allows multiple input payloads at a time (usually stored in S3).\n",
    "\n",
    "If you want to learn more about batch transform, check out this [AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_transform_inference_instance_type = \"ml.m5.xlarge\"  # replace with a large instance if necessary\n",
    "content_type = \"text/csv\"  # label inspector only supports csv file, do not change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch transform jobs can take in multiple payloads, we upload data from a folder (instead of an individual file) to an S3 folder. We will then pass in that S3 folder to the batch transform job which can perform inference on all files in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speficy location of local folder that we want to upload to S3 and download results to\n",
    "# replace this local folder if you have your data stored somewhere else\n",
    "transform_input_folder = \"data/batch_inference/input/\"  \n",
    "transform_output_folder = \"data/batch_inference/output/\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input and output folders paths on S3\n",
    "transform_input_path = \"{}/{}/{}\".format(base_folder_name, \"inference\", \"input\")\n",
    "transform_output_path = \"s3://{}/{}/{}/{}\".format(bucket_name, base_folder_name, \"inference\", \"output\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we upload the data in our specified local folder to a S3 folder. Our local folder (specified using `transform_input_folder`) contains one csv file that we want to perform batch transform on. Note that you can have as many files as you want in that folder as long as they are valid files to perform batch transform on.\n",
    "\n",
    "Skip this step if your data is already in S3, and just specify where your data is using the `transform_input_location` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the batch inference input files to S3 (skip if data is already in S3), just specify transform_input_location\n",
    "transform_input_location = sagemaker_session.upload_data(transform_input_folder, bucket=bucket_name, key_prefix=transform_input_path)\n",
    "\n",
    "print(\"Transform input uploaded to \" + transform_input_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring that your data is on S3, you can run the batch transform by passing in the S3 location of where your data is stored. The batch transform will save the results of the batch transform to an output folder in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(1, batch_transform_inference_instance_type, output_path=transform_output_path)\n",
    "transformer.transform(transform_input_location, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-043170249292/label-inspector/inference/output'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the batch transform job is complete, we can get the output data from the S3 bucket specified above. Here, we will download the entire S3 folder, so if you had more than one input files, the follow code cell will download all the corresponding output files/\n",
    "\n",
    "Then, we will show a sample of what the output data will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource = session.resource('s3')\n",
    "resource_bucket = resource.Bucket(bucket_name)\n",
    "\n",
    "for object in resource_bucket.objects.filter(Prefix=f'{base_folder_name}/inference/output/'):\n",
    "    filename = object.key.split(\"/\")[-1]\n",
    "    resource_bucket.download_file(object.key, f\"{transform_output_folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter_grade\n",
       "0            B\n",
       "1            A\n",
       "2            C\n",
       "3            F\n",
       "4            D"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transform_preds = pd.read_csv(f\"{transform_output_folder}/inference_dataset.csv.out\")\n",
    "batch_transform_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean Up Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete models endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are done with performing real-time inferences, you no longer need the saved model and deployed endpoint. Here we show how to terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also delete the model associated with the batch transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformer.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete S3 resources (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains the code that can be used to automatically delete the files created in S3 in this sample notebook. Be wary that this command will delete all the files stored in the `base_folder_name` folder specified above. Proceed with caution if you have previously stored other files in that folder.\n",
    "\n",
    "Alternatively, you can navigate to the [S3 Management Console](https://s3.console.aws.amazon.com/) and manually delete these data files yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resource = session.resource('s3')\n",
    "# resource_bucket = resource.Bucket(bucket_name)\n",
    "\n",
    "# before executing the code below, ensure there is nothing else important in this folder\n",
    "# resource_bucket.objects.filter(Prefix=f\"{base_folder_name}/\").delete()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get results faster (but potentially less accurate estimates): Specify a more powerful instance type and set `hyperparameters = {\"runtime\": \"fast\"}`. You can also try subsampling your dataset to a smaller one for a quick trial run (although be aware the ML training -- and hence accuracy of estimated label issues -- will become worse when the dataset is small). \n",
    "\n",
    "To get more accurate results: Specify a more powerful instance type. If you have text fields in your dataset (i.e. strings that are not discrete categories), use an instance that has a GPU (like a p-instance or g-instance) so large language models can be fine-tuned on your data. Also set `hyperparameters = {\"runtime\": \"high_accuracy\"}`.\n",
    "\n",
    "To ask questions or report problems, please email: support@cleanlab.ai\n",
    "\n",
    "To run a more in-depth analysis of your data and labels, try [Cleanlab Studio](https://cleanlab.ai/studio/) for free (it supports image data as well)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
